{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import random\n",
    "import os\n",
    "\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "#game_url = \"chrome://dino\"\n",
    "#chrome_driver_path = \"/usr/local/bin/chromedriver\"\n",
    "#chrome_driver_path = \"./chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "#init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "#getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "#return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,env_name):\n",
    "        self.env = retro.make(game=env_name)\n",
    "    def restart(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def Run(self):\n",
    "        act = [0,0,0,0,0,0,0,1,0]\n",
    "        return act\n",
    "    def Stop(self):\n",
    "        act = [0,0,0,0,0,0,1,1,0]\n",
    "        return act\n",
    "    def Jump(self):\n",
    "        act = [0,0,0,0,0,0,0,1,1]\n",
    "        return act\n",
    "\n",
    "    def get_score(self, act):\n",
    "        obs, rew, done, info = self.env.step(act)\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "    \n",
    "    def get_action_sample(self):\n",
    "        act = self.env.action_space.sample()\n",
    "        return act\n",
    "    \n",
    "    def end(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clown_agent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game\n",
    "        \n",
    "    def Run(self):\n",
    "        self._game.Run()\n",
    "    def Jump(self):\n",
    "        self._game.Jump()\n",
    "    def Stop(self):\n",
    "        self._game.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state_:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions, t, tt):\n",
    "        \n",
    "        image, reward, is_over, info = self._game.get_score(actions)\n",
    "        \n",
    "        reward_1 = 0.2 \n",
    "        \n",
    "        image_ap = process_img(image)\n",
    "        \n",
    "        actions_df.loc[len(actions_df)] = actions[8] # storing actions in a dataframe\n",
    "        \n",
    "        self._display.send(image_ap) #display the image on screen\n",
    "        \n",
    "        if reward > 50:\n",
    "            reward_1 = reward_1 + 8\n",
    "        elif actions[8]==1:\n",
    "            reward_1 = reward_1 - 0.2\n",
    "        \n",
    "        #delay = 200\n",
    "        score_base = 8180\n",
    "        \n",
    "        if info['dead'] < 100:\n",
    "            score = info['score'] - score_base\n",
    "            reward_1 = -10\n",
    "            is_over = True\n",
    "            scores_df.loc[len(scores_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "        \n",
    "        \n",
    "        '''if t % delay == 0:\n",
    "            if t % (2*delay) == 0 or info['lives']<2:\n",
    "                tt[0] = info['bonus']\n",
    "                if tt[0]-tt[1] == 0:\n",
    "                    score = info['score'] - score_base\n",
    "                    reward_1 = -10\n",
    "                    is_over = True\n",
    "                    scores_df.loc[len(scores_df)] = score # log the score when game is over\n",
    "                    self._game.restart()\n",
    "                    tt[0]=0\n",
    "                    tt[1]=1\n",
    "\n",
    "            elif t % (2*delay) == delay:\n",
    "                tt[1] = info['bonus']\n",
    "                if tt[0]-tt[1] == 0 or info['lives']<2:\n",
    "                    score = info['score'] - score_base\n",
    "                    reward_1 = -10\n",
    "                    is_over = True\n",
    "                    scores_df.loc[len(scores_df)] = score # log the score when game is over\n",
    "                    self._game.restart()\n",
    "                    tt[0]=0\n",
    "                    tt[1]=1'''\n",
    "\n",
    "        \n",
    "        return image_ap, reward_1, is_over, info, tt #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[35:135, 0:120] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (150, 150)) \n",
    "        cv2.imshow(window_title, imS)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(q_value_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "Key_num = 9\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 200. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call only once to init file structure\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "#init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(Key_num)\n",
    "    #do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    tt = [0, 1]\n",
    "    \n",
    "    x_t, r_0, terminal, info, tt = game_state.get_state(do_nothing, 1, tt) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 9999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros(Key_num) # action at t\n",
    "        #a_t = [0,0,0,0,0,0,0,1,0]\n",
    "        #run, jump\n",
    "        \n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        \n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            \n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                #action_index = random.randrange(ACTIONS)\n",
    "                action_index = random.choice([0,1])\n",
    "                a_t_all = [[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1,1]]\n",
    "                #a_t = random.choice([[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1,1]])\n",
    "                a_t = a_t_all[action_index]\n",
    "\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       # input a stack of 4 images, get the prediction\n",
    "                print(q)\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q\n",
    "                a_t_all = [[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1,1]]\n",
    "                a_t = a_t_all[action_index]       # run, jump\n",
    "                \n",
    "        #else:\n",
    "        #    x_t1, r_t, terminal, info, tt = game_state.get_state(a_t, t, tt)\n",
    "\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal, info, tt = game_state.get_state(a_t, t, tt)\n",
    "        #if a_t[8]==1:\n",
    "        #    for ii in range(0, 30):\n",
    "        #        x_t1, r_t, terminal, info, tt = game_state.get_state([0,0,0,0,0,0,0,1,1], t, tt)\n",
    "\n",
    "            \n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        \n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                 \n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 100 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            #game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            #game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\ndef main():\\n    \\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    \\n    game.restart()\\n    \\n    while True:\\n        obs, rew, done, info = game.get_score(game.Jump())\\n        game.render()\\n        if info['lives']<3:\\n            game.restart()\\n    game.end()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "def main():\n",
    "    \n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    \n",
    "    game.restart()\n",
    "    \n",
    "    while True:\n",
    "        obs, rew, done, info = game.get_score(game.Jump())\n",
    "        game.render()\n",
    "        if info['lives']<3:\n",
    "            game.restart()\n",
    "    game.end()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\n#main function\\ndef main(observe=True):\\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    game.restart()\\n    \\n    clown = clown_agent(game)\\n    game_state = Game_state_(clown,game)  \\n    model = buildmodel()\\n    try:\\n        trainNetwork(model,game_state,observe=observe)\\n    except StopIteration:\\n        game.restart()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=True):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)  \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.restart()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=False):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)\n",
    "    model = buildmodel()\n",
    "    \n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 5700 / STATE explore / EPSILON 0.09450649899999161 / ACTION 1 / REWARD 0.0 / Q_MAX  29.95162 / Loss  0.6462101936340332\n",
      "[[51.077816 50.622658]]\n",
      "fps: 3.3692733934253325\n",
      "TIMESTEP 5701 / STATE explore / EPSILON 0.09450549999999161 / ACTION 0 / REWARD 0.2 / Q_MAX  41.62412 / Loss  0.8056755065917969\n",
      "[[51.102383 50.349716]]\n",
      "fps: 13.800321128688374\n",
      "TIMESTEP 5702 / STATE explore / EPSILON 0.09450450099999161 / ACTION 0 / REWARD 0.2 / Q_MAX  40.60981 / Loss  1.974215030670166\n",
      "[[51.688572 50.350388]]\n",
      "fps: 13.860427613099368\n",
      "TIMESTEP 5703 / STATE explore / EPSILON 0.09450350199999161 / ACTION 0 / REWARD 0.2 / Q_MAX  41.71878 / Loss  1.4151923656463623\n",
      "[[49.090626 47.555397]]\n",
      "fps: 13.859236838974875\n",
      "TIMESTEP 5704 / STATE explore / EPSILON 0.0945025029999916 / ACTION 0 / REWARD 0.2 / Q_MAX  32.121593 / Loss  1.5221447944641113\n",
      "----------Random Action----------\n",
      "fps: 14.619036830750034\n",
      "TIMESTEP 5705 / STATE explore / EPSILON 0.0945015039999916 / ACTION 0 / REWARD 0.2 / Q_MAX  44.123917 / Loss  45.164283752441406\n",
      "[[47.568714 46.01015 ]]\n",
      "fps: 13.810090447231433\n",
      "TIMESTEP 5706 / STATE explore / EPSILON 0.0945005049999916 / ACTION 0 / REWARD 0.2 / Q_MAX  53.49428 / Loss  0.8882531523704529\n",
      "[[46.377747 44.945248]]\n",
      "fps: 13.082669993761696\n",
      "TIMESTEP 5707 / STATE explore / EPSILON 0.0944995059999916 / ACTION 0 / REWARD 0.2 / Q_MAX  42.192642 / Loss  0.4475199580192566\n",
      "----------Random Action----------\n",
      "fps: 14.003699326241845\n",
      "TIMESTEP 5708 / STATE explore / EPSILON 0.0944985069999916 / ACTION 1 / REWARD 0.0 / Q_MAX  38.249638 / Loss  1.0273017883300781\n",
      "[[42.931465 40.582752]]\n",
      "fps: 13.743029866151149\n",
      "TIMESTEP 5709 / STATE explore / EPSILON 0.0944975079999916 / ACTION 0 / REWARD 0.2 / Q_MAX  39.93947 / Loss  1.3168777227401733\n",
      "[[41.837368 39.236805]]\n",
      "fps: 13.806408289827976\n",
      "TIMESTEP 5710 / STATE explore / EPSILON 0.0944965089999916 / ACTION 0 / REWARD 0.2 / Q_MAX  37.39511 / Loss  1.5633865594863892\n",
      "[[41.90901 39.69997]]\n",
      "fps: 11.35252530720511\n",
      "TIMESTEP 5711 / STATE explore / EPSILON 0.0944955099999916 / ACTION 0 / REWARD 0.2 / Q_MAX  46.900192 / Loss  0.5203713774681091\n",
      "[[41.924496 40.4072  ]]\n",
      "fps: 13.075981095135365\n",
      "TIMESTEP 5712 / STATE explore / EPSILON 0.0944945109999916 / ACTION 0 / REWARD 0.2 / Q_MAX  40.956696 / Loss  1.721239686012268\n",
      "[[42.23148  41.747566]]\n",
      "fps: 12.873663694541213\n",
      "TIMESTEP 5713 / STATE explore / EPSILON 0.09449351199999159 / ACTION 0 / REWARD 0.2 / Q_MAX  43.583046 / Loss  0.7255846261978149\n",
      "[[42.185787 41.619904]]\n",
      "fps: 13.443023531598751\n",
      "TIMESTEP 5714 / STATE explore / EPSILON 0.09449251299999159 / ACTION 0 / REWARD 0.2 / Q_MAX  38.33959 / Loss  0.6389734148979187\n",
      "[[42.60847 42.90966]]\n",
      "fps: 13.2770634302609\n",
      "TIMESTEP 5715 / STATE explore / EPSILON 0.09449151399999159 / ACTION 1 / REWARD 0.0 / Q_MAX  46.132053 / Loss  0.3792189359664917\n",
      "[[42.620262 42.792927]]\n",
      "fps: 13.357698591396789\n",
      "TIMESTEP 5716 / STATE explore / EPSILON 0.09449051499999159 / ACTION 1 / REWARD 0.0 / Q_MAX  32.700504 / Loss  1.6119053363800049\n",
      "[[42.348427 42.896442]]\n",
      "fps: 13.367362288540724\n",
      "TIMESTEP 5717 / STATE explore / EPSILON 0.09448951599999159 / ACTION 1 / REWARD 0.0 / Q_MAX  44.0602 / Loss  2.4310805797576904\n",
      "[[41.60259  42.615646]]\n",
      "fps: 10.977180948093789\n",
      "TIMESTEP 5718 / STATE explore / EPSILON 0.09448851699999158 / ACTION 1 / REWARD 0.0 / Q_MAX  42.115353 / Loss  0.7884151339530945\n",
      "[[42.671898 43.20467 ]]\n",
      "fps: 12.886082871723028\n",
      "TIMESTEP 5719 / STATE explore / EPSILON 0.09448751799999158 / ACTION 1 / REWARD 0.0 / Q_MAX  45.716286 / Loss  1.370102882385254\n",
      "[[40.791435 41.500698]]\n",
      "fps: 13.865376096686964\n",
      "TIMESTEP 5720 / STATE explore / EPSILON 0.09448651899999158 / ACTION 1 / REWARD 0.0 / Q_MAX  41.334446 / Loss  1.6016793251037598\n",
      "[[41.45143 42.17558]]\n",
      "fps: 14.379123327870989\n",
      "TIMESTEP 5721 / STATE explore / EPSILON 0.09448551999999158 / ACTION 1 / REWARD 0.0 / Q_MAX  16.531994 / Loss  1.775733470916748\n",
      "[[41.25131  41.390614]]\n",
      "fps: 14.339157695371393\n",
      "TIMESTEP 5722 / STATE explore / EPSILON 0.09448452099999158 / ACTION 1 / REWARD 0.0 / Q_MAX  50.5433 / Loss  2.0265328884124756\n",
      "----------Random Action----------\n",
      "fps: 13.873631426095356\n",
      "TIMESTEP 5723 / STATE explore / EPSILON 0.09448352199999158 / ACTION 0 / REWARD 0.2 / Q_MAX  42.568436 / Loss  0.6514409780502319\n",
      "[[41.978664 41.722626]]\n",
      "fps: 14.939161344783248\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
