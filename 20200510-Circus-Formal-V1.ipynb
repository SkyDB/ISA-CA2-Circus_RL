{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import random\n",
    "import os\n",
    "\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "#game_url = \"chrome://dino\"\n",
    "#chrome_driver_path = \"/usr/local/bin/chromedriver\"\n",
    "#chrome_driver_path = \"./chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "#init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "#getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "#return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,env_name):\n",
    "        self.env = retro.make(game=env_name)\n",
    "    def restart(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def Run(self):\n",
    "        act = [0,0,0,0,0,0,0,1,0]\n",
    "        return act\n",
    "    def Jump(self):\n",
    "        act = [0,0,0,0,0,0,0,1,1]\n",
    "        return act\n",
    "    def Stop(self):\n",
    "        act = [0,0,0,0,0,0,1,1,0]\n",
    "        return act\n",
    "    def get_score(self, act):\n",
    "        obs, rew, done, info = self.env.step(act)\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "    \n",
    "    def get_action_sample(self):\n",
    "        act = self.env.action_space.sample()\n",
    "        return act\n",
    "    \n",
    "    def end(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clown_agent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game\n",
    "        \n",
    "    def Run(self):\n",
    "        self._game.Run()\n",
    "    def Jump(self):\n",
    "        self._game.Jump()\n",
    "    def Stop(self):\n",
    "        self._game.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state_:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        \n",
    "        image, reward, is_over, info = self._game.get_score(actions)\n",
    "        \n",
    "        image_ap = process_img(image)\n",
    "        \n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        \n",
    "        self._display.send(image) #display the image on screen\n",
    "        \n",
    "        if is_over:\n",
    "            score = info['score']\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "        \n",
    "        return image_ap, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[50:224, 0:240] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 3 # possible actions: jump, do nothing\n",
    "Key_num = 9\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call only once to init file structure\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "#init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(Key_num)\n",
    "    #do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 9999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros(Key_num) # action at t\n",
    "        #run, jump, stop\n",
    "        \n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                #action_index = random.randrange(ACTIONS)\n",
    "                a_t = random.choice([[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1,1], [0,0,0,0,0,0,1,1,0]])\n",
    "                \n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       # input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t_all = [[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1,1], [0,0,0,0,0,0,1,1,0]]\n",
    "                a_t = a_t_all[action_index-1]       # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        \n",
    "            \n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        \n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                \n",
    "                #could not broadcast input array from shape (3) into shape (9)\n",
    "                \n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            #game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            #game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\ndef main():\\n    \\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    \\n    game.restart()\\n    \\n    while True:\\n        obs, rew, done, info = game.get_score(game.Jump())\\n        game.render()\\n        if info['lives']<3:\\n            game.restart()\\n    game.end()\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "def main():\n",
    "    \n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    \n",
    "    game.restart()\n",
    "    \n",
    "    while True:\n",
    "        obs, rew, done, info = game.get_score(game.Jump())\n",
    "        game.render()\n",
    "        if info['lives']<3:\n",
    "            game.restart()\n",
    "    game.end()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\n#main function\\ndef main(observe=True):\\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    game.restart()\\n    \\n    clown = clown_agent(game)\\n    game_state = Game_state_(clown,game)  \\n    model = buildmodel()\\n    try:\\n        trainNetwork(model,game_state,observe=observe)\\n    except StopIteration:\\n        game.restart()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=True):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)  \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.restart()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=False):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)\n",
    "    model = buildmodel()\n",
    "    \n",
    "    trainNetwork(model,game_state,observe=observe)\n",
    "\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 24000 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  164.02602 / Loss  84.75204467773438\n",
      "fps: 0.9119665958280444\n",
      "TIMESTEP 24001 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  101.45297 / Loss  24.759746551513672\n",
      "fps: 20.493008257194507\n",
      "TIMESTEP 24002 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  199.21237 / Loss  58.74580001831055\n",
      "fps: 19.87049582626657\n",
      "TIMESTEP 24003 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  190.93837 / Loss  34.12130355834961\n",
      "fps: 19.572570521944048\n",
      "TIMESTEP 24004 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  112.15446 / Loss  54.87868881225586\n",
      "fps: 20.481700141124996\n",
      "TIMESTEP 24005 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  1.3153442 / Loss  18.20986557006836\n",
      "fps: 21.081989625638347\n",
      "TIMESTEP 24006 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  172.18013 / Loss  64.85089874267578\n",
      "fps: 20.94157354982375\n",
      "TIMESTEP 24007 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  107.7138 / Loss  123.36312866210938\n",
      "fps: 23.941184528973924\n",
      "TIMESTEP 24008 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  74.91959 / Loss  46.23330307006836\n",
      "fps: 23.976219874697033\n",
      "TIMESTEP 24009 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  131.29558 / Loss  46.37724685668945\n",
      "fps: 23.95841568322804\n",
      "TIMESTEP 24010 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  254.44476 / Loss  36.275882720947266\n",
      "fps: 20.018059811192884\n",
      "TIMESTEP 24011 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  142.0938 / Loss  19.54141616821289\n",
      "fps: 20.87383047338456\n",
      "TIMESTEP 24012 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  152.88737 / Loss  21.508495330810547\n",
      "fps: 21.341583050088534\n",
      "TIMESTEP 24013 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  270.86832 / Loss  38.10770034790039\n",
      "fps: 21.474451657826293\n",
      "TIMESTEP 24014 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  123.991684 / Loss  7.598677635192871\n",
      "fps: 18.97025314451897\n",
      "TIMESTEP 24015 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  123.12317 / Loss  241.87188720703125\n",
      "fps: 21.142137045960904\n",
      "TIMESTEP 24016 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  187.10486 / Loss  51.491859436035156\n",
      "fps: 21.488203861858384\n",
      "TIMESTEP 24017 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  77.70514 / Loss  58.24032211303711\n",
      "fps: 22.222655504927413\n",
      "TIMESTEP 24018 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  113.43144 / Loss  249.27206420898438\n",
      "fps: 19.905576384604434\n",
      "TIMESTEP 24019 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  99.72495 / Loss  53.27958297729492\n",
      "fps: 21.472143014380276\n",
      "TIMESTEP 24020 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  186.6953 / Loss  176.66970825195312\n",
      "fps: 21.12775978359972\n",
      "TIMESTEP 24021 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  125.93283 / Loss  38.88219451904297\n",
      "fps: 21.20434369375746\n",
      "TIMESTEP 24022 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  128.91891 / Loss  139.45762634277344\n",
      "fps: 23.927526641261437\n",
      "TIMESTEP 24023 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  322.23074 / Loss  25.789213180541992\n",
      "fps: 20.003166700050553\n",
      "TIMESTEP 24024 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  120.63398 / Loss  257.07232666015625\n",
      "fps: 21.855008727822213\n",
      "TIMESTEP 24025 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  241.34666 / Loss  59.16350555419922\n",
      "fps: 21.425089137030945\n",
      "TIMESTEP 24026 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  100.734764 / Loss  36.76439666748047\n",
      "fps: 20.197063596396188\n",
      "TIMESTEP 24027 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  98.56931 / Loss  27.90723419189453\n",
      "fps: 20.42783321887952\n",
      "TIMESTEP 24028 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  157.08263 / Loss  36.231300354003906\n",
      "fps: 19.79453683952202\n",
      "TIMESTEP 24029 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  86.43851 / Loss  9.299031257629395\n",
      "fps: 22.070173223043085\n",
      "TIMESTEP 24030 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  85.99869 / Loss  4.4886155128479\n",
      "fps: 19.51637879690292\n",
      "TIMESTEP 24031 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  108.332985 / Loss  3.1849188804626465\n",
      "fps: 21.44173729896633\n",
      "TIMESTEP 24032 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  156.77425 / Loss  111.01786804199219\n",
      "fps: 20.650507855387993\n",
      "TIMESTEP 24033 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  1.2754084 / Loss  212.61329650878906\n",
      "fps: 20.684726270262807\n",
      "TIMESTEP 24034 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  82.29279 / Loss  9.124732971191406\n",
      "fps: 22.415648130828636\n",
      "TIMESTEP 24035 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  76.310905 / Loss  4.679440975189209\n",
      "fps: 21.951431920951265\n",
      "TIMESTEP 24036 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  101.53018 / Loss  12.588006019592285\n",
      "fps: 18.611159674306126\n",
      "TIMESTEP 24037 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  70.3941 / Loss  28.440532684326172\n",
      "fps: 20.05011687883322\n",
      "TIMESTEP 24038 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  64.589554 / Loss  92.66279602050781\n",
      "fps: 21.7226906424147\n",
      "TIMESTEP 24039 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  140.10469 / Loss  35.58246612548828\n",
      "fps: 22.45813633467373\n",
      "TIMESTEP 24040 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  56.584553 / Loss  39.764163970947266\n",
      "fps: 21.608651077005508\n",
      "TIMESTEP 24041 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  59.330246 / Loss  7.380107879638672\n",
      "fps: 19.2363087676171\n",
      "TIMESTEP 24042 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  110.22822 / Loss  4.603683948516846\n",
      "fps: 20.79105365428084\n",
      "TIMESTEP 24043 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  56.516975 / Loss  34.534881591796875\n",
      "fps: 22.34685733466176\n",
      "TIMESTEP 24044 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  99.84884 / Loss  1.4238225221633911\n",
      "fps: 23.344876911174563\n",
      "TIMESTEP 24045 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  99.461876 / Loss  135.4813232421875\n",
      "fps: 23.53771991357782\n",
      "TIMESTEP 24046 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  124.45894 / Loss  56.62441635131836\n",
      "fps: 20.879857425900298\n",
      "TIMESTEP 24047 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  51.198547 / Loss  7.672231674194336\n",
      "fps: 20.269976126269803\n",
      "TIMESTEP 24048 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  58.432034 / Loss  11.994022369384766\n",
      "fps: 21.77705319778611\n",
      "TIMESTEP 24049 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  51.72287 / Loss  52.15702438354492\n",
      "fps: 21.581965905641056\n",
      "TIMESTEP 24050 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  39.76876 / Loss  23.675731658935547\n",
      "fps: 22.80963438708309\n",
      "TIMESTEP 24051 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  111.83511 / Loss  9.971356391906738\n",
      "fps: 22.213945999766967\n",
      "TIMESTEP 24052 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  39.05241 / Loss  113.88472747802734\n",
      "fps: 22.077956805297482\n",
      "TIMESTEP 24053 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  50.930798 / Loss  8.663030624389648\n",
      "fps: 24.204796749844185\n",
      "TIMESTEP 24054 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  45.92761 / Loss  53.01491165161133\n",
      "fps: 23.536002873047227\n",
      "TIMESTEP 24055 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  97.556145 / Loss  14.442041397094727\n",
      "fps: 23.965808058875965\n",
      "TIMESTEP 24056 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  48.55998 / Loss  15.36497974395752\n",
      "fps: 22.38741186329403\n",
      "TIMESTEP 24057 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  1.2571164 / Loss  8.490736961364746\n",
      "fps: 23.02551068022991\n",
      "TIMESTEP 24058 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  34.91342 / Loss  111.51107025146484\n",
      "fps: 21.872217934544544\n",
      "TIMESTEP 24059 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  1.2587211 / Loss  18.106210708618164\n",
      "fps: 23.222860180166215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 24060 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  48.121113 / Loss  12.866823196411133\n",
      "fps: 21.683833945096417\n",
      "TIMESTEP 24061 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  95.93142 / Loss  67.94381713867188\n",
      "fps: 20.949418363626375\n",
      "TIMESTEP 24062 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  92.907715 / Loss  116.70711517333984\n",
      "fps: 22.296488849905643\n",
      "TIMESTEP 24063 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  55.775932 / Loss  13.545966148376465\n",
      "fps: 20.86292845737934\n",
      "TIMESTEP 24064 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  108.07022 / Loss  7.08403205871582\n",
      "fps: 22.80752582925503\n",
      "TIMESTEP 24065 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  138.22574 / Loss  39.56094741821289\n",
      "fps: 22.143462766940317\n",
      "TIMESTEP 24066 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  47.310505 / Loss  0.7087094783782959\n",
      "fps: 23.117137077883783\n",
      "TIMESTEP 24067 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  139.99309 / Loss  6.2342987060546875\n",
      "fps: 22.679759484362158\n",
      "TIMESTEP 24068 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  103.95759 / Loss  2.509639263153076\n",
      "fps: 21.312953515315353\n",
      "TIMESTEP 24069 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  58.418045 / Loss  11.070775985717773\n",
      "fps: 22.63667371875135\n",
      "TIMESTEP 24070 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  62.20926 / Loss  1.5054479837417603\n",
      "fps: 21.309596752478065\n",
      "TIMESTEP 24071 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  98.01217 / Loss  1.9019927978515625\n",
      "fps: 22.05462251154182\n",
      "TIMESTEP 24072 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  107.00904 / Loss  12.426386833190918\n",
      "fps: 23.034488818593207\n",
      "TIMESTEP 24073 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  61.433758 / Loss  2.429105281829834\n",
      "fps: 21.748484612791994\n",
      "TIMESTEP 24074 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  62.084644 / Loss  52.00130081176758\n",
      "fps: 21.795951879855537\n",
      "TIMESTEP 24075 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  55.950687 / Loss  1.2178716659545898\n",
      "fps: 22.215828557505905\n",
      "TIMESTEP 24076 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  1.2809696 / Loss  4.058239936828613\n",
      "fps: 22.313450939501628\n",
      "TIMESTEP 24077 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  171.97627 / Loss  2.3539915084838867\n",
      "fps: 24.588774636823036\n",
      "TIMESTEP 24078 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  68.80584 / Loss  24.868858337402344\n",
      "fps: 24.350519892944433\n",
      "TIMESTEP 24079 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  114.08759 / Loss  2.839540481567383\n",
      "fps: 24.047839922025055\n",
      "TIMESTEP 24080 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  75.58717 / Loss  247.88613891601562\n",
      "fps: 23.047652540882716\n",
      "TIMESTEP 24081 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  65.14909 / Loss  4.3639373779296875\n",
      "fps: 21.739692017436106\n",
      "TIMESTEP 24082 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  58.452335 / Loss  8.265913009643555\n",
      "fps: 22.929968619818716\n",
      "TIMESTEP 24083 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  169.00137 / Loss  95.51828002929688\n",
      "fps: 22.750618355391627\n",
      "TIMESTEP 24084 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  56.967464 / Loss  9.14057731628418\n",
      "fps: 21.758864512380487\n",
      "TIMESTEP 24085 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  63.582916 / Loss  1.3169794082641602\n",
      "fps: 20.231843791001005\n",
      "TIMESTEP 24086 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  58.66994 / Loss  97.767822265625\n",
      "fps: 21.760332036316473\n",
      "TIMESTEP 24087 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  65.617325 / Loss  3.7353923320770264\n",
      "fps: 23.498027967013268\n",
      "TIMESTEP 24088 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  53.521015 / Loss  6.2376627922058105\n",
      "fps: 22.904299296101527\n",
      "TIMESTEP 24089 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  106.04712 / Loss  2.825305700302124\n",
      "fps: 23.85716317139623\n",
      "TIMESTEP 24090 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  111.57929 / Loss  36.91518783569336\n",
      "fps: 21.785083960505062\n",
      "TIMESTEP 24091 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  86.727875 / Loss  2.1258771419525146\n",
      "fps: 21.99632896655164\n",
      "TIMESTEP 24092 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  52.720825 / Loss  12.263919830322266\n",
      "fps: 23.06057773721424\n",
      "TIMESTEP 24093 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  97.551315 / Loss  13.31678581237793\n",
      "fps: 21.22065043612005\n",
      "TIMESTEP 24094 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  47.548534 / Loss  1.7696499824523926\n",
      "fps: 21.84886101401789\n",
      "TIMESTEP 24095 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  169.872 / Loss  14.223603248596191\n",
      "fps: 21.801843198203585\n",
      "TIMESTEP 24096 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  66.03424 / Loss  13.028804779052734\n",
      "fps: 24.13543404955634\n",
      "TIMESTEP 24097 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  46.72787 / Loss  26.243030548095703\n",
      "fps: 23.302983499083282\n",
      "TIMESTEP 24098 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  107.89364 / Loss  9.35996150970459\n",
      "fps: 24.130712937819304\n",
      "TIMESTEP 24099 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  47.364433 / Loss  3.3285651206970215\n",
      "fps: 23.332929088390568\n",
      "TIMESTEP 24100 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  56.59569 / Loss  11.531083106994629\n",
      "fps: 23.85336419524901\n",
      "TIMESTEP 24101 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  173.3738 / Loss  4.767317295074463\n",
      "fps: 21.205308526504716\n",
      "TIMESTEP 24102 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  56.31384 / Loss  105.53621673583984\n",
      "fps: 21.423775909448455\n",
      "TIMESTEP 24103 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  62.77188 / Loss  0.5331730842590332\n",
      "fps: 22.779036550263402\n",
      "TIMESTEP 24104 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  56.905895 / Loss  97.31682586669922\n",
      "fps: 22.906300687576117\n",
      "TIMESTEP 24105 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  114.13669 / Loss  15.351836204528809\n",
      "fps: 22.447799536519184\n",
      "TIMESTEP 24106 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  47.454365 / Loss  5.636624336242676\n",
      "fps: 21.975584453689052\n",
      "TIMESTEP 24107 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  68.64141 / Loss  22.035106658935547\n",
      "fps: 23.251439943677276\n",
      "TIMESTEP 24108 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  180.59294 / Loss  35.07766342163086\n",
      "fps: 21.88191716359121\n",
      "TIMESTEP 24109 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  49.054848 / Loss  24.805381774902344\n",
      "fps: 21.225053261205094\n",
      "TIMESTEP 24110 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  50.224075 / Loss  1.3670774698257446\n",
      "fps: 20.556182335903078\n",
      "TIMESTEP 24111 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  51.987034 / Loss  16.218847274780273\n",
      "fps: 22.150947182745274\n",
      "TIMESTEP 24112 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  61.711617 / Loss  17.661272048950195\n",
      "fps: 22.13142814930508\n",
      "TIMESTEP 24113 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  71.561745 / Loss  6.573878288269043\n",
      "fps: 23.19486365571894\n",
      "TIMESTEP 24114 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  190.47174 / Loss  7.780183792114258\n",
      "fps: 20.94063256696373\n",
      "TIMESTEP 24115 / STATE explore / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  72.403725 / Loss  45.78226089477539\n",
      "fps: 21.466208781366593\n",
      "TIMESTEP 24116 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  72.73835 / Loss  5.671758651733398\n",
      "fps: 21.988026463403127\n",
      "TIMESTEP 24117 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  69.501945 / Loss  11.896232604980469\n",
      "fps: 20.692379796544614\n",
      "TIMESTEP 24118 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  70.61991 / Loss  253.4883575439453\n",
      "fps: 21.060183371996104\n",
      "TIMESTEP 24119 / STATE explore / EPSILON 0.0001 / ACTION 2 / REWARD 0.0 / Q_MAX  134.19536 / Loss  0.888831377029419\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
