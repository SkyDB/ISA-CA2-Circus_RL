{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import random\n",
    "import os\n",
    "\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "#game_url = \"chrome://dino\"\n",
    "#chrome_driver_path = \"/usr/local/bin/chromedriver\"\n",
    "#chrome_driver_path = \"./chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "#init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "#getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "#return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,env_name):\n",
    "        self.env = retro.make(game=env_name)\n",
    "    def restart(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def Run(self):\n",
    "        act = [0,0,0,0,0,0,0,1,0]\n",
    "        return act\n",
    "    def Stop(self):\n",
    "        act = [0,0,0,0,0,0,1,1,0]\n",
    "        return act\n",
    "    def Jump(self):\n",
    "        act = [0,0,0,0,0,0,0,1,1]\n",
    "        return act\n",
    "\n",
    "    def get_score(self, act):\n",
    "        obs, rew, done, info = self.env.step(act)\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "    \n",
    "    def get_action_sample(self):\n",
    "        act = self.env.action_space.sample()\n",
    "        return act\n",
    "    \n",
    "    def end(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clown_agent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game\n",
    "        \n",
    "    def Run(self):\n",
    "        self._game.Run()\n",
    "    def Jump(self):\n",
    "        self._game.Jump()\n",
    "    def Stop(self):\n",
    "        self._game.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state_:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        \n",
    "        image, reward, is_over, info = self._game.get_score(actions)\n",
    "        \n",
    "        self._game.render()\n",
    "        \n",
    "        reward_1 = 0.2 \n",
    "        \n",
    "        image_ap = process_img(image)\n",
    "        \n",
    "        actions_df.loc[len(actions_df)] = actions[8] # storing actions in a dataframe\n",
    "        \n",
    "        self._display.send(image_ap) #display the image on screen\n",
    "        \n",
    "        if reward > 50:\n",
    "            reward_1 = reward_1 + 8\n",
    "        elif actions[8]==1:\n",
    "            reward_1 = reward_1 - 0.2\n",
    "        \n",
    "        #delay = 200\n",
    "        score_base = 8180\n",
    "        \n",
    "        if info['dead'] < 100:\n",
    "            score = info['score'] - score_base\n",
    "            reward_1 = -10\n",
    "            is_over = True\n",
    "            scores_df.loc[len(scores_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "        \n",
    "        \n",
    "        '''if t % delay == 0:\n",
    "            if t % (2*delay) == 0 or info['lives']<2:\n",
    "                tt[0] = info['bonus']\n",
    "                if tt[0]-tt[1] == 0:\n",
    "                    score = info['score'] - score_base\n",
    "                    reward_1 = -10\n",
    "                    is_over = True\n",
    "                    scores_df.loc[len(scores_df)] = score # log the score when game is over\n",
    "                    self._game.restart()\n",
    "                    tt[0]=0\n",
    "                    tt[1]=1\n",
    "\n",
    "            elif t % (2*delay) == delay:\n",
    "                tt[1] = info['bonus']\n",
    "                if tt[0]-tt[1] == 0 or info['lives']<2:\n",
    "                    score = info['score'] - score_base\n",
    "                    reward_1 = -10\n",
    "                    is_over = True\n",
    "                    scores_df.loc[len(scores_df)] = score # log the score when game is over\n",
    "                    self._game.restart()\n",
    "                    tt[0]=0\n",
    "                    tt[1]=1'''\n",
    "\n",
    "        \n",
    "        return image_ap, reward_1, is_over, info #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[35:135, 30:150] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (150, 150)) \n",
    "        cv2.imshow(window_title, imS)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(q_value_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "Key_num = 9\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 1000. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call only once to init file structure\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "#init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  \n",
    "    #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(Key_num)\n",
    "    #do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "        \n",
    "    \n",
    "    x_t, r_0, terminal, info = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 9999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros(Key_num) # action at t\n",
    "        a_t_fill = [0,0,0,0,0,0,0,1,0]\n",
    "        #run, jump\n",
    "        \n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        \n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            \n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                #action_index = random.randrange(ACTIONS)\n",
    "                action_index = random.choice([0,1])\n",
    "                a_t_all = [[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1,1]]\n",
    "                #a_t = random.choice([[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1,1]])\n",
    "                a_t = a_t_all[action_index]\n",
    "\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       # input a stack of 4 images, get the prediction\n",
    "                print(q)\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q\n",
    "                a_t_all = [[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1,1]]\n",
    "                a_t = a_t_all[action_index]       # run, jump\n",
    "               \n",
    "        terminal_transfer = False\n",
    "        \n",
    "        for i in range(0, 5):\n",
    "            A1, A2, A3, A4 = game_state.get_state(a_t_fill)\n",
    "            if A3 == True:\n",
    "                terminal_transfer = True\n",
    "\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal, info = game_state.get_state(a_t)\n",
    "        \n",
    "        if terminal == False:\n",
    "            terminal = terminal_transfer\n",
    "            \n",
    "        #if a_t[8]==1:\n",
    "        #    for ii in range(0, 30):\n",
    "        #        x_t1, r_t, terminal, info = game_state.get_state([0,0,0,0,0,0,0,1,1])\n",
    "\n",
    "            \n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        \n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                 \n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 100 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            #game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            #game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\ndef main():\\n    \\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    \\n    game.restart()\\n    \\n    while True:\\n        obs, rew, done, info = game.get_score(game.Jump())\\n        game.render()\\n        if info['lives']<3:\\n            game.restart()\\n    game.end()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "def main():\n",
    "    \n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    \n",
    "    game.restart()\n",
    "    \n",
    "    while True:\n",
    "        obs, rew, done, info = game.get_score(game.Jump())\n",
    "        game.render()\n",
    "        if info['lives']<3:\n",
    "            game.restart()\n",
    "    game.end()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\n#main function\\ndef main(observe=True):\\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    game.restart()\\n    \\n    clown = clown_agent(game)\\n    game_state = Game_state_(clown,game)  \\n    model = buildmodel()\\n    try:\\n        trainNetwork(model,game_state,observe=observe)\\n    except StopIteration:\\n        game.restart()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=True):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)  \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.restart()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=False):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)\n",
    "    model = buildmodel()\n",
    "    \n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 90800 / STATE explore / EPSILON 0.010290798999872196 / ACTION 0 / REWARD 0.2 / Q_MAX  14.199382 / Loss  0.25859710574150085\n",
      "[[14.718991 14.695465]]\n",
      "fps: 0.36001135404155554\n",
      "TIMESTEP 90801 / STATE explore / EPSILON 0.010289799999872196 / ACTION 0 / REWARD 0.2 / Q_MAX  14.247369 / Loss  0.09985947608947754\n",
      "[[14.422144 14.70145 ]]\n",
      "fps: 3.631979189938571\n",
      "TIMESTEP 90802 / STATE explore / EPSILON 0.010288800999872196 / ACTION 1 / REWARD 0.0 / Q_MAX  14.049167 / Loss  1.348071575164795\n",
      "[[14.030674 14.373833]]\n",
      "fps: 3.5400885548809\n",
      "TIMESTEP 90803 / STATE explore / EPSILON 0.010287801999872196 / ACTION 1 / REWARD 0.0 / Q_MAX  14.354919 / Loss  0.3150983452796936\n",
      "[[15.052415 14.979464]]\n",
      "fps: 3.5048742130481507\n",
      "TIMESTEP 90804 / STATE explore / EPSILON 0.010286802999872197 / ACTION 0 / REWARD 0.2 / Q_MAX  14.914672 / Loss  6.394308567047119\n",
      "[[14.20164 14.18495]]\n",
      "fps: 3.4616322302965776\n",
      "TIMESTEP 90805 / STATE explore / EPSILON 0.010285803999872197 / ACTION 0 / REWARD 0.2 / Q_MAX  14.1496725 / Loss  0.14324107766151428\n",
      "[[13.9272175 14.025729 ]]\n",
      "fps: 3.552914169543212\n",
      "TIMESTEP 90806 / STATE explore / EPSILON 0.010284804999872197 / ACTION 1 / REWARD 0.0 / Q_MAX  13.494756 / Loss  0.08638494461774826\n",
      "[[14.042565 13.95958 ]]\n",
      "fps: 3.485969035801316\n",
      "TIMESTEP 90807 / STATE explore / EPSILON 0.010283805999872197 / ACTION 0 / REWARD 0.2 / Q_MAX  14.7061405 / Loss  0.18797093629837036\n",
      "[[14.134488 14.137209]]\n",
      "fps: 3.518265723050184\n",
      "TIMESTEP 90808 / STATE explore / EPSILON 0.010282806999872197 / ACTION 1 / REWARD 0.0 / Q_MAX  13.977084 / Loss  0.07253992557525635\n",
      "[[13.594335 13.703049]]\n",
      "fps: 3.4103858806923055\n",
      "TIMESTEP 90809 / STATE explore / EPSILON 0.010281807999872198 / ACTION 1 / REWARD 0.0 / Q_MAX  13.052808 / Loss  0.044991955161094666\n",
      "[[12.99402  12.849958]]\n",
      "fps: 3.6115046587099924\n",
      "TIMESTEP 90810 / STATE explore / EPSILON 0.010280808999872198 / ACTION 0 / REWARD 0.2 / Q_MAX  14.278909 / Loss  0.0946861132979393\n",
      "[[12.7339945 12.772175 ]]\n",
      "fps: 3.5470011238967167\n",
      "TIMESTEP 90811 / STATE explore / EPSILON 0.010279809999872198 / ACTION 1 / REWARD 0.0 / Q_MAX  14.128482 / Loss  1.8008294105529785\n",
      "[[12.283136 12.245611]]\n",
      "fps: 3.4558423314121867\n",
      "TIMESTEP 90812 / STATE explore / EPSILON 0.010278810999872198 / ACTION 0 / REWARD 0.2 / Q_MAX  14.278318 / Loss  4.522906303405762\n",
      "[[12.232592  12.3673115]]\n",
      "fps: 3.4543964592120036\n",
      "TIMESTEP 90813 / STATE explore / EPSILON 0.010277811999872198 / ACTION 1 / REWARD 0.0 / Q_MAX  14.0689335 / Loss  0.12165522575378418\n",
      "[[12.951359 13.086572]]\n",
      "fps: 3.3827542043441934\n",
      "TIMESTEP 90814 / STATE explore / EPSILON 0.010276812999872199 / ACTION 1 / REWARD 0.0 / Q_MAX  14.23774 / Loss  2.3617236614227295\n",
      "[[12.739802 12.909234]]\n",
      "fps: 3.5195765402066113\n",
      "TIMESTEP 90815 / STATE explore / EPSILON 0.010275813999872199 / ACTION 1 / REWARD 0.0 / Q_MAX  13.993179 / Loss  0.06958155333995819\n",
      "[[13.918236 14.114196]]\n",
      "fps: 3.6138259750979427\n",
      "TIMESTEP 90816 / STATE explore / EPSILON 0.010274814999872199 / ACTION 1 / REWARD 0.0 / Q_MAX  13.9377985 / Loss  0.08224561810493469\n",
      "[[14.0821085 14.449247 ]]\n",
      "fps: 3.5797259156273498\n",
      "TIMESTEP 90817 / STATE explore / EPSILON 0.0102738159998722 / ACTION 1 / REWARD 0.0 / Q_MAX  13.52369 / Loss  0.08347943425178528\n",
      "[[13.962326  14.2248955]]\n",
      "fps: 3.5198364577939762\n",
      "TIMESTEP 90818 / STATE explore / EPSILON 0.0102728169998722 / ACTION 1 / REWARD 0.0 / Q_MAX  13.283033 / Loss  0.4285321533679962\n",
      "[[14.119674 14.188367]]\n",
      "fps: 3.5054512753351625\n",
      "TIMESTEP 90819 / STATE explore / EPSILON 0.0102718179998722 / ACTION 1 / REWARD 0.0 / Q_MAX  10.295105 / Loss  0.5439136624336243\n",
      "[[14.327021 14.248953]]\n",
      "fps: 3.3387760300579505\n",
      "TIMESTEP 90820 / STATE explore / EPSILON 0.0102708189998722 / ACTION 0 / REWARD 0.2 / Q_MAX  9.596444 / Loss  0.6048657298088074\n",
      "[[13.999128 14.255884]]\n",
      "fps: 3.665756848761345\n",
      "TIMESTEP 90821 / STATE explore / EPSILON 0.0102698199998722 / ACTION 1 / REWARD 0.0 / Q_MAX  1.9811678 / Loss  3.042732000350952\n",
      "[[13.600863 13.77269 ]]\n",
      "fps: 3.8219574402942165\n",
      "TIMESTEP 90822 / STATE explore / EPSILON 0.0102688209998722 / ACTION 1 / REWARD 0.0 / Q_MAX  13.588546 / Loss  1.5540623664855957\n",
      "[[13.322872 13.529661]]\n",
      "fps: 3.7318029957248418\n",
      "TIMESTEP 90823 / STATE explore / EPSILON 0.0102678219998722 / ACTION 1 / REWARD 0.0 / Q_MAX  0.51712793 / Loss  0.14433442056179047\n",
      "[[13.298009 13.375747]]\n",
      "fps: 3.763486419141603\n",
      "TIMESTEP 90824 / STATE explore / EPSILON 0.0102668229998722 / ACTION 1 / REWARD 0.0 / Q_MAX  13.775083 / Loss  0.1580725908279419\n",
      "[[13.0359535 13.181076 ]]\n",
      "fps: 3.6492739828407808\n",
      "TIMESTEP 90825 / STATE explore / EPSILON 0.010265823999872201 / ACTION 1 / REWARD 0.0 / Q_MAX  12.040605 / Loss  0.24960118532180786\n",
      "[[12.866323 13.121748]]\n",
      "fps: 3.7909814803098363\n",
      "TIMESTEP 90826 / STATE explore / EPSILON 0.010264824999872201 / ACTION 1 / REWARD 0.0 / Q_MAX  13.825436 / Loss  0.11868428438901901\n",
      "[[13.01347  13.699814]]\n",
      "fps: 3.7437777817448277\n",
      "TIMESTEP 90827 / STATE explore / EPSILON 0.010263825999872201 / ACTION 1 / REWARD 0.0 / Q_MAX  13.046567 / Loss  0.703436017036438\n",
      "[[13.434209  13.5568695]]\n",
      "fps: 3.7773897192920205\n",
      "TIMESTEP 90828 / STATE explore / EPSILON 0.010262826999872202 / ACTION 1 / REWARD 0.0 / Q_MAX  13.531163 / Loss  0.40181484818458557\n",
      "[[13.232331 13.360429]]\n",
      "fps: 3.804554067647942\n",
      "TIMESTEP 90829 / STATE explore / EPSILON 0.010261827999872202 / ACTION 1 / REWARD 0.0 / Q_MAX  14.521543 / Loss  0.07939653843641281\n",
      "[[13.334465 13.548848]]\n",
      "fps: 3.7438312488173064\n",
      "TIMESTEP 90830 / STATE explore / EPSILON 0.010260828999872202 / ACTION 1 / REWARD 0.0 / Q_MAX  14.173763 / Loss  0.31190225481987\n",
      "[[12.965064 13.085898]]\n",
      "fps: 3.6484740410836767\n",
      "TIMESTEP 90831 / STATE explore / EPSILON 0.010259829999872202 / ACTION 1 / REWARD 0.0 / Q_MAX  14.421991 / Loss  0.5014656186103821\n",
      "[[13.768165 13.718454]]\n",
      "fps: 3.679485155046512\n",
      "TIMESTEP 90832 / STATE explore / EPSILON 0.010258830999872202 / ACTION 0 / REWARD 0.2 / Q_MAX  12.067474 / Loss  0.11146983504295349\n",
      "[[13.742402 13.584417]]\n",
      "fps: 3.482620128816053\n",
      "TIMESTEP 90833 / STATE explore / EPSILON 0.010257831999872203 / ACTION 0 / REWARD 0.2 / Q_MAX  14.064353 / Loss  0.09068754315376282\n",
      "[[12.427821 12.420089]]\n",
      "fps: 3.54697712729207\n",
      "TIMESTEP 90834 / STATE explore / EPSILON 0.010256832999872203 / ACTION 0 / REWARD 0.2 / Q_MAX  13.257873 / Loss  0.09667228907346725\n",
      "[[10.003463  9.737695]]\n",
      "fps: 3.441011521712735\n",
      "TIMESTEP 90835 / STATE explore / EPSILON 0.010255833999872203 / ACTION 0 / REWARD 0.2 / Q_MAX  14.362605 / Loss  0.17709475755691528\n",
      "[[8.853743 7.903169]]\n",
      "fps: 3.604927236285252\n",
      "TIMESTEP 90836 / STATE explore / EPSILON 0.010254834999872203 / ACTION 0 / REWARD 0.2 / Q_MAX  15.775523 / Loss  0.16775789856910706\n",
      "[[9.366631 9.580925]]\n",
      "fps: 3.603929837585731\n",
      "TIMESTEP 90837 / STATE explore / EPSILON 0.010253835999872203 / ACTION 1 / REWARD 0.0 / Q_MAX  1.5339446 / Loss  0.2920774221420288\n",
      "[[7.4946437 7.808837 ]]\n",
      "fps: 3.480146596980275\n",
      "TIMESTEP 90838 / STATE explore / EPSILON 0.010252836999872204 / ACTION 1 / REWARD 0.0 / Q_MAX  14.686564 / Loss  13.030406951904297\n",
      "[[7.6574597 8.29082  ]]\n",
      "fps: 3.542004845624877\n",
      "TIMESTEP 90839 / STATE explore / EPSILON 0.010251837999872204 / ACTION 1 / REWARD 0.0 / Q_MAX  14.375782 / Loss  0.1975080966949463\n",
      "[[ 8.837261 10.093787]]\n",
      "fps: 3.405081110394893\n",
      "TIMESTEP 90840 / STATE explore / EPSILON 0.010250838999872204 / ACTION 1 / REWARD 0.0 / Q_MAX  14.2226 / Loss  0.2633925676345825\n",
      "[[ 9.785816 10.141012]]\n",
      "fps: 3.413891758010547\n",
      "TIMESTEP 90841 / STATE explore / EPSILON 0.010249839999872204 / ACTION 1 / REWARD 0.0 / Q_MAX  13.496812 / Loss  0.16090577840805054\n",
      "[[8.650985 8.240751]]\n",
      "fps: 3.572487536806135\n",
      "TIMESTEP 90842 / STATE explore / EPSILON 0.010248840999872205 / ACTION 0 / REWARD 0.2 / Q_MAX  14.223543 / Loss  0.23807284235954285\n",
      "[[6.876829  7.0459375]]\n",
      "fps: 3.51298806809711\n",
      "TIMESTEP 90843 / STATE explore / EPSILON 0.010247841999872205 / ACTION 1 / REWARD 0.0 / Q_MAX  14.667625 / Loss  0.17389829456806183\n",
      "[[ 9.405932 10.411672]]\n",
      "fps: 3.566651757090694\n",
      "TIMESTEP 90844 / STATE explore / EPSILON 0.010246842999872205 / ACTION 1 / REWARD 0.0 / Q_MAX  14.702558 / Loss  0.2722941040992737\n",
      "[[7.2551637 7.869605 ]]\n",
      "fps: 3.603295135216865\n",
      "TIMESTEP 90845 / STATE explore / EPSILON 0.010245843999872205 / ACTION 1 / REWARD 0.0 / Q_MAX  14.916977 / Loss  0.44982320070266724\n",
      "[[3.9413705 5.3487296]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 3.5836500341763498\n",
      "TIMESTEP 90846 / STATE explore / EPSILON 0.010244844999872205 / ACTION 1 / REWARD 0.0 / Q_MAX  15.258469 / Loss  0.11579997837543488\n",
      "[[2.4896429 3.2532594]]\n",
      "fps: 3.46030426030426\n",
      "TIMESTEP 90847 / STATE explore / EPSILON 0.010243845999872206 / ACTION 1 / REWARD 0.0 / Q_MAX  14.607589 / Loss  0.2983451783657074\n",
      "[[-0.27720857  1.4248879 ]]\n",
      "fps: 3.5272869779043345\n",
      "TIMESTEP 90848 / STATE explore / EPSILON 0.010242846999872206 / ACTION 1 / REWARD 0.0 / Q_MAX  15.064971 / Loss  0.1533220261335373\n",
      "[[-2.623389  -1.8360355]]\n",
      "fps: 3.455161937650133\n",
      "TIMESTEP 90849 / STATE explore / EPSILON 0.010241847999872206 / ACTION 1 / REWARD 0.0 / Q_MAX  14.816483 / Loss  0.17210832238197327\n",
      "[[14.8478565 14.385069 ]]\n",
      "fps: 3.34277275415266\n",
      "TIMESTEP 90850 / STATE explore / EPSILON 0.010240848999872206 / ACTION 0 / REWARD 0.2 / Q_MAX  1.730381 / Loss  0.5131662487983704\n",
      "[[15.5571575 15.012563 ]]\n",
      "fps: 3.4580048576747466\n",
      "TIMESTEP 90851 / STATE explore / EPSILON 0.010239849999872206 / ACTION 0 / REWARD 0.2 / Q_MAX  14.340281 / Loss  0.37972140312194824\n",
      "[[14.401569 13.737734]]\n",
      "fps: 3.473454511726821\n",
      "TIMESTEP 90852 / STATE explore / EPSILON 0.010238850999872207 / ACTION 0 / REWARD 0.2 / Q_MAX  13.824803 / Loss  0.08068685233592987\n",
      "[[14.728407 14.332613]]\n",
      "fps: 3.550279710276189\n",
      "TIMESTEP 90853 / STATE explore / EPSILON 0.010237851999872207 / ACTION 0 / REWARD 0.2 / Q_MAX  13.958717 / Loss  0.18300074338912964\n",
      "[[15.00972  14.396152]]\n",
      "fps: 3.444227108717681\n",
      "TIMESTEP 90854 / STATE explore / EPSILON 0.010236852999872207 / ACTION 0 / REWARD 0.2 / Q_MAX  12.920467 / Loss  0.04094075784087181\n",
      "[[15.039756 14.357522]]\n",
      "fps: 3.5226507495378643\n",
      "TIMESTEP 90855 / STATE explore / EPSILON 0.010235853999872207 / ACTION 0 / REWARD 0.2 / Q_MAX  2.063709 / Loss  0.09355908632278442\n",
      "[[15.212349 14.667781]]\n",
      "fps: 3.5408147673522383\n",
      "TIMESTEP 90856 / STATE explore / EPSILON 0.010234854999872207 / ACTION 0 / REWARD 0.2 / Q_MAX  14.900087 / Loss  0.5657171607017517\n",
      "[[14.63615  14.040222]]\n",
      "fps: 3.4977629782702424\n",
      "TIMESTEP 90857 / STATE explore / EPSILON 0.010233855999872208 / ACTION 0 / REWARD 0.2 / Q_MAX  13.752564 / Loss  0.22721290588378906\n",
      "[[14.550745 13.989324]]\n",
      "fps: 3.3668663039961984\n",
      "TIMESTEP 90858 / STATE explore / EPSILON 0.010232856999872208 / ACTION 0 / REWARD 0.2 / Q_MAX  14.587333 / Loss  0.352314829826355\n",
      "[[14.891696 14.327727]]\n",
      "fps: 3.530139941303263\n",
      "TIMESTEP 90859 / STATE explore / EPSILON 0.010231857999872208 / ACTION 0 / REWARD 0.2 / Q_MAX  14.966075 / Loss  0.1662154495716095\n",
      "[[15.021342 14.404155]]\n",
      "fps: 3.5278328698867374\n",
      "TIMESTEP 90860 / STATE explore / EPSILON 0.010230858999872208 / ACTION 0 / REWARD 0.2 / Q_MAX  14.311228 / Loss  3.7915687561035156\n",
      "[[15.23122  14.706661]]\n",
      "fps: 3.566700284533004\n",
      "TIMESTEP 90861 / STATE explore / EPSILON 0.010229859999872208 / ACTION 0 / REWARD 0.2 / Q_MAX  15.2455015 / Loss  0.17258866131305695\n",
      "[[14.85718  14.870069]]\n",
      "fps: 3.4499639318183872\n",
      "TIMESTEP 90862 / STATE explore / EPSILON 0.010228860999872209 / ACTION 1 / REWARD 0.0 / Q_MAX  12.916199 / Loss  0.7778459787368774\n",
      "[[14.190607 14.166009]]\n",
      "fps: 3.3982062271626146\n",
      "TIMESTEP 90863 / STATE explore / EPSILON 0.010227861999872209 / ACTION 0 / REWARD 0.2 / Q_MAX  14.776482 / Loss  0.2928192615509033\n",
      "[[14.7207155 14.253352 ]]\n",
      "fps: 3.4758695673792483\n",
      "TIMESTEP 90864 / STATE explore / EPSILON 0.010226862999872209 / ACTION 0 / REWARD 0.2 / Q_MAX  13.792804 / Loss  0.31244516372680664\n",
      "[[14.428471 14.065515]]\n",
      "fps: 3.5493243317573806\n",
      "TIMESTEP 90865 / STATE explore / EPSILON 0.01022586399987221 / ACTION 0 / REWARD 0.2 / Q_MAX  12.952938 / Loss  0.08726781606674194\n",
      "[[14.373832 14.008967]]\n",
      "fps: 3.335526106512791\n",
      "TIMESTEP 90866 / STATE explore / EPSILON 0.01022486499987221 / ACTION 0 / REWARD 0.2 / Q_MAX  -10.264759 / Loss  0.24053891003131866\n",
      "[[14.104418 13.712314]]\n",
      "fps: 3.5330600197614643\n",
      "TIMESTEP 90867 / STATE explore / EPSILON 0.01022386599987221 / ACTION 0 / REWARD 0.2 / Q_MAX  14.298714 / Loss  0.08076481521129608\n",
      "[[14.377254 14.020342]]\n",
      "fps: 3.43545916653971\n",
      "TIMESTEP 90868 / STATE explore / EPSILON 0.01022286699987221 / ACTION 0 / REWARD 0.2 / Q_MAX  14.718197 / Loss  0.12979422509670258\n",
      "[[14.281364 14.038896]]\n",
      "fps: 3.6439539856007355\n",
      "TIMESTEP 90869 / STATE explore / EPSILON 0.01022186799987221 / ACTION 0 / REWARD 0.2 / Q_MAX  13.891618 / Loss  0.09517641365528107\n",
      "[[14.257418 14.055373]]\n",
      "fps: 3.536020705382472\n",
      "TIMESTEP 90870 / STATE explore / EPSILON 0.01022086899987221 / ACTION 0 / REWARD 0.2 / Q_MAX  14.162184 / Loss  0.25812625885009766\n",
      "[[14.964906  14.7226515]]\n",
      "fps: 3.4662179807148785\n",
      "TIMESTEP 90871 / STATE explore / EPSILON 0.01021986999987221 / ACTION 0 / REWARD 0.2 / Q_MAX  13.403953 / Loss  0.1869153380393982\n",
      "[[14.882031 14.830508]]\n",
      "fps: 3.5990128745053216\n",
      "TIMESTEP 90872 / STATE explore / EPSILON 0.01021887099987221 / ACTION 0 / REWARD 0.2 / Q_MAX  14.395154 / Loss  0.1841936558485031\n",
      "[[14.854255  14.5040455]]\n",
      "fps: 3.5348644350484135\n",
      "TIMESTEP 90873 / STATE explore / EPSILON 0.010217871999872211 / ACTION 0 / REWARD 0.2 / Q_MAX  13.948297 / Loss  1.930039405822754\n",
      "[[14.782491 14.372579]]\n",
      "fps: 3.406206577871332\n",
      "TIMESTEP 90874 / STATE explore / EPSILON 0.010216872999872211 / ACTION 0 / REWARD 0.2 / Q_MAX  13.127737 / Loss  0.11997747421264648\n",
      "[[14.86695  14.750598]]\n",
      "fps: 3.433749162910338\n",
      "TIMESTEP 90875 / STATE explore / EPSILON 0.010215873999872211 / ACTION 0 / REWARD 0.2 / Q_MAX  13.250906 / Loss  1.896336317062378\n",
      "[[15.117834 15.039298]]\n",
      "fps: 3.464222559754334\n",
      "TIMESTEP 90876 / STATE explore / EPSILON 0.010214874999872212 / ACTION 0 / REWARD 0.2 / Q_MAX  13.189789 / Loss  0.15544965863227844\n",
      "[[14.480656 14.659938]]\n",
      "fps: 3.0830609327163736\n",
      "TIMESTEP 90877 / STATE explore / EPSILON 0.010213875999872212 / ACTION 1 / REWARD 0.0 / Q_MAX  13.992872 / Loss  0.806481122970581\n",
      "[[13.52163  13.899452]]\n",
      "fps: 3.5395478247367262\n",
      "TIMESTEP 90878 / STATE explore / EPSILON 0.010212876999872212 / ACTION 1 / REWARD 0.0 / Q_MAX  14.210038 / Loss  0.19946585595607758\n",
      "[[13.992193 14.039013]]\n",
      "fps: 3.486835531221709\n",
      "TIMESTEP 90879 / STATE explore / EPSILON 0.010211877999872212 / ACTION 1 / REWARD 0.0 / Q_MAX  14.349733 / Loss  0.0850801095366478\n",
      "[[13.615534 13.54725 ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
