{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import random\n",
    "import os\n",
    "\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "#game_url = \"chrome://dino\"\n",
    "#chrome_driver_path = \"/usr/local/bin/chromedriver\"\n",
    "#chrome_driver_path = \"./chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "#init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "#getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "#return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,env_name):\n",
    "        self.env = retro.make(game=env_name)\n",
    "    def restart(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def Run(self):\n",
    "        act = [0,0,0,0,0,0,0,1,0]\n",
    "        return act\n",
    "    def Stop(self):\n",
    "        act = [0,0,0,0,0,0,1,1,0]\n",
    "        return act\n",
    "    def Jump(self):\n",
    "        act = [0,0,0,0,0,0,0,1,1]\n",
    "        return act\n",
    "\n",
    "    def get_score(self, act):\n",
    "        obs, rew, done, info = self.env.step(act)\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "    \n",
    "    def get_action_sample(self):\n",
    "        act = self.env.action_space.sample()\n",
    "        return act\n",
    "    \n",
    "    def end(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clown_agent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game\n",
    "        \n",
    "    def Run(self):\n",
    "        self._game.Run()\n",
    "    def Jump(self):\n",
    "        self._game.Jump()\n",
    "    def Stop(self):\n",
    "        self._game.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state_:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        \n",
    "        image, reward, is_over, info = self._game.get_score(actions)\n",
    "        \n",
    "        #reward_1 = 0.1 \n",
    "        \n",
    "        image_ap = process_img(image)\n",
    "        \n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        \n",
    "        self._display.send(image) #display the image on screen\n",
    "        \n",
    "        if is_over:\n",
    "            score = info['score']\n",
    "            #reward_1 = -1\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "        \n",
    "        return image_ap, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[50:224, 0:240] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 3 # possible actions: jump, do nothing\n",
    "Key_num = 9\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call only once to init file structure\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "#init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(Key_num)\n",
    "    #do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 9999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros(Key_num) # action at t\n",
    "        #run, stop, jump\n",
    "        \n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                #action_index = random.randrange(ACTIONS)\n",
    "                a_t = random.choice([[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,1,1,0], [0,0,0,0,0,0,0,1,1]])\n",
    "                \n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       # input a stack of 4 images, get the prediction\n",
    "                print(q)\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q\n",
    "                a_t_all = [[0,0,0,0,0,0,0,1,0], [0,0,0,0,0,0,1,1,0], [0,0,0,0,0,0,0,1,1]]\n",
    "                a_t = a_t_all[action_index]       # run, jump, stop\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        \n",
    "            \n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        \n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                 \n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            #game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            #game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\ndef main():\\n    \\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    \\n    game.restart()\\n    \\n    while True:\\n        obs, rew, done, info = game.get_score(game.Jump())\\n        game.render()\\n        if info['lives']<3:\\n            game.restart()\\n    game.end()\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "def main():\n",
    "    \n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    \n",
    "    game.restart()\n",
    "    \n",
    "    while True:\n",
    "        obs, rew, done, info = game.get_score(game.Jump())\n",
    "        game.render()\n",
    "        if info['lives']<3:\n",
    "            game.restart()\n",
    "    game.end()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import retro\\n\\n#main function\\ndef main(observe=True):\\n    env_name = 'CircusCharlie-Nes'\\n    game = Game(env_name)\\n    game.restart()\\n    \\n    clown = clown_agent(game)\\n    game_state = Game_state_(clown,game)  \\n    model = buildmodel()\\n    try:\\n        trainNetwork(model,game_state,observe=observe)\\n    except StopIteration:\\n        game.restart()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=True):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)  \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.restart()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "\n",
    "#main function\n",
    "def main(observe=False):\n",
    "    env_name = 'CircusCharlie-Nes'\n",
    "    game = Game(env_name)\n",
    "    game.restart()\n",
    "    \n",
    "    clown = clown_agent(game)\n",
    "    game_state = Game_state_(clown,game)\n",
    "    model = buildmodel()\n",
    "    \n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 310000 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  63.82657 / Loss  0.20664343237876892\n",
      "[[23.778332 24.108706 23.609188]]\n",
      "fps: 0.32336936011353035\n",
      "TIMESTEP 310001 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  24.108706 / Loss  1.7399216890335083\n",
      "[[23.745247 24.085415 23.567535]]\n",
      "fps: 13.04174673358084\n",
      "TIMESTEP 310002 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  35.3785 / Loss  4.132396697998047\n",
      "[[23.706285 24.054342 23.51012 ]]\n",
      "fps: 13.787029823713683\n",
      "TIMESTEP 310003 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  73.76246 / Loss  3.717404365539551\n",
      "[[23.651388 24.016476 23.451723]]\n",
      "fps: 12.911907400566433\n",
      "TIMESTEP 310004 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  49.71812 / Loss  0.8447373509407043\n",
      "[[23.591677 23.97712  23.396225]]\n",
      "fps: 12.905352225350379\n",
      "TIMESTEP 310005 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  51.764076 / Loss  38.83916091918945\n",
      "[[23.651093 24.052229 23.482407]]\n",
      "fps: 13.231159424861673\n",
      "TIMESTEP 310006 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  75.78506 / Loss  0.546860933303833\n",
      "[[23.693872 24.104841 23.549164]]\n",
      "fps: 13.265767375662213\n",
      "TIMESTEP 310007 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  81.62023 / Loss  1.113145351409912\n",
      "[[23.727047 24.150263 23.609236]]\n",
      "fps: 12.855591961086608\n",
      "TIMESTEP 310008 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  79.401245 / Loss  2.0791680812835693\n",
      "[[23.76197  24.205719 23.687042]]\n",
      "fps: 12.928264735889824\n",
      "TIMESTEP 310009 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  38.732258 / Loss  0.8064096570014954\n",
      "[[23.805124 24.27     23.770613]]\n",
      "fps: 13.106831370367708\n",
      "TIMESTEP 310010 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  35.681248 / Loss  0.6760229468345642\n",
      "[[23.851482 24.335545 23.860495]]\n",
      "fps: 12.549229867036873\n",
      "TIMESTEP 310011 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  24.335545 / Loss  1.2052823305130005\n",
      "[[23.906776 24.405136 23.951174]]\n",
      "fps: 13.579028816923023\n",
      "TIMESTEP 310012 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  37.701763 / Loss  3.2582907676696777\n",
      "[[23.977383 24.484238 24.049892]]\n",
      "fps: 13.404657733006497\n",
      "TIMESTEP 310013 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  83.692154 / Loss  0.5861956477165222\n",
      "[[24.027725 24.537914 24.11343 ]]\n",
      "fps: 12.522179429765636\n",
      "TIMESTEP 310014 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  50.543053 / Loss  2.483360528945923\n",
      "[[24.085415 24.600136 24.18132 ]]\n",
      "fps: 12.481004597461725\n",
      "TIMESTEP 310015 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  24.600136 / Loss  4.259252548217773\n",
      "[[24.12349  24.63957  24.216856]]\n",
      "fps: 12.451585131616838\n",
      "TIMESTEP 310016 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  69.52606 / Loss  12.73299789428711\n",
      "[[24.140255 24.662373 24.234724]]\n",
      "fps: 13.313940532836451\n",
      "TIMESTEP 310017 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  49.21469 / Loss  0.4775336682796478\n",
      "[[24.160273 24.684296 24.249168]]\n",
      "fps: 13.590820866262925\n",
      "TIMESTEP 310018 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  30.009071 / Loss  2.2770304679870605\n",
      "[[24.206985 24.730488 24.295923]]\n",
      "fps: 13.536388105365752\n",
      "TIMESTEP 310019 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  24.730488 / Loss  0.622205376625061\n",
      "[[24.233253 24.757816 24.322289]]\n",
      "fps: 12.398179130948861\n",
      "TIMESTEP 310020 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  24.757816 / Loss  3.4360294342041016\n",
      "[[24.249023 24.77024  24.336933]]\n",
      "fps: 14.112732166890982\n",
      "TIMESTEP 310021 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  35.853405 / Loss  11.712574005126953\n",
      "[[24.27045  24.77371  24.339994]]\n",
      "fps: 12.599064000048061\n",
      "TIMESTEP 310022 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  59.73499 / Loss  13.687751770019531\n",
      "[[24.323034 24.814102 24.401226]]\n",
      "fps: 13.06575705185116\n",
      "TIMESTEP 310023 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  48.70331 / Loss  0.1229630559682846\n",
      "[[24.37286  24.853262 24.458553]]\n",
      "fps: 12.792270295658751\n",
      "TIMESTEP 310024 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  78.107216 / Loss  2.9721643924713135\n",
      "[[24.389412 24.859558 24.477428]]\n",
      "fps: 13.42233116897663\n",
      "TIMESTEP 310025 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  213.70374 / Loss  6.600834846496582\n",
      "[[24.42731  24.865225 24.50106 ]]\n",
      "fps: 12.677166734672289\n",
      "TIMESTEP 310026 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  47.970364 / Loss  0.8879830837249756\n",
      "[[24.471828 24.880762 24.53923 ]]\n",
      "fps: 13.056524364809086\n",
      "TIMESTEP 310027 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  47.88189 / Loss  0.7425557374954224\n",
      "[[24.495174 24.877413 24.551222]]\n",
      "fps: 12.837492309999602\n",
      "TIMESTEP 310028 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  82.21365 / Loss  1.0479609966278076\n",
      "[[24.515806 24.869572 24.558952]]\n",
      "fps: 13.146145455913143\n",
      "TIMESTEP 310029 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  56.39854 / Loss  7.337923049926758\n",
      "[[24.60161  24.929947 24.626436]]\n",
      "fps: 12.591915219381859\n",
      "TIMESTEP 310030 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  65.848206 / Loss  2.0982613563537598\n",
      "[[24.661007 24.974306 24.681597]]\n",
      "fps: 12.634701642030684\n",
      "TIMESTEP 310031 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  134.03792 / Loss  16.126436233520508\n",
      "[[24.669796 24.984533 24.699053]]\n",
      "fps: 12.670885961228812\n",
      "TIMESTEP 310032 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  48.588734 / Loss  0.3877963423728943\n",
      "[[24.673214 24.98557  24.71098 ]]\n",
      "fps: 13.153360073006207\n",
      "TIMESTEP 310033 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  39.71559 / Loss  3.3655784130096436\n",
      "[[24.6604   24.961199 24.70544 ]]\n",
      "fps: 12.69531633079284\n",
      "TIMESTEP 310034 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  47.18262 / Loss  0.13963256776332855\n",
      "[[24.64342  24.935335 24.698145]]\n",
      "fps: 12.992701815253083\n",
      "TIMESTEP 310035 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  83.391174 / Loss  1.6446654796600342\n",
      "[[24.626717 24.905188 24.676224]]\n",
      "fps: 13.623576108175177\n",
      "TIMESTEP 310036 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  62.026287 / Loss  12.539043426513672\n",
      "[[24.630484 24.902168 24.696974]]\n",
      "fps: 12.993587320863202\n",
      "TIMESTEP 310037 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  84.39587 / Loss  3.7458150386810303\n",
      "[[24.658278 24.917152 24.733889]]\n",
      "fps: 13.162895501591107\n",
      "TIMESTEP 310038 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  55.75779 / Loss  0.44126489758491516\n",
      "[[24.679314 24.920887 24.760893]]\n",
      "fps: 13.678223068669878\n",
      "TIMESTEP 310039 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  37.92524 / Loss  4.162032127380371\n",
      "[[24.70887  24.936214 24.805115]]\n",
      "fps: 13.129725685629408\n",
      "TIMESTEP 310040 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  56.41515 / Loss  1.581652045249939\n",
      "[[24.736528 24.95178  24.841068]]\n",
      "fps: 12.373600179365848\n",
      "TIMESTEP 310041 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  40.848804 / Loss  0.22120767831802368\n",
      "[[24.76211  24.96539  24.873951]]\n",
      "fps: 12.325168085006347\n",
      "TIMESTEP 310042 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  45.806564 / Loss  18.458189010620117\n",
      "[[24.82604  25.019331 24.945644]]\n",
      "fps: 13.112199025878617\n",
      "TIMESTEP 310043 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  50.214165 / Loss  0.6911090612411499\n",
      "[[24.876389 25.056627 24.99303 ]]\n",
      "fps: 13.460409559599876\n",
      "TIMESTEP 310044 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.0 / Q_MAX  25.056627 / Loss  12.199626922607422\n",
      "[[24.93552  25.112461 25.07462 ]]\n",
      "fps: 12.985300491634778\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
